{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T22:34:15.182502Z",
     "start_time": "2020-04-01T22:34:15.178512Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"devre.png\" height=2000 width=800>\n",
    "\n",
    "\n",
    "# Ka|Ve Girişim Odaklı Yapay Zeka Eğitimi 5. Hafta / LSTM ve RNN'ler\n",
    "\n",
    "\n",
    "Ders İçerikleri: https://github.com/yemregundogmus/lstm_lecture_notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Referanslar: \n",
    "\n",
    "[1][Vanilla LSTM with numpy](https://blog.varunajayasiri.com/numpy_lstm.html)\n",
    "\n",
    "[2][Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[3][MIT 6.S191 (2019): Recurrent Neural Networks](https://www.youtube.com/watch?v=_h66BW-xNgk)\n",
    "\n",
    "[4][MIT 6.S191 (2019): Recurrent Neural Networks](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf)\n",
    "\n",
    "[5][Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "[6][Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sıralı Veriler \n",
    "Aşağıdaki topun nereye hareket edeceğini tahmin edebilir misiniz?\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/138/1*R_suE2YyL7gOhRY2DDxrJg.png\" height=200 width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T00:53:34.463561Z",
     "start_time": "2020-04-02T00:53:34.457555Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Topun hareket anlarında topun yerini kaydedersek, t-1, t-n anında topun nerede olduğunu da bilip buna göre t+1'de gideceği yönü de tahmin edebiliriz.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/480/1*2UsTgXbxwHXYmFmskHL-9w.gif\" height=1000 width=400></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Şimdi sizi kafanızdan alfabeyi saymaya davet ediyorum :)\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*IRnAWr8sOIrHGkTTVfglaQ.png\" height=1000 width=400></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T01:12:47.212354Z",
     "start_time": "2020-04-02T01:12:47.206399Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Oldukça kolay oldu değil mi? Şimdi ise alfabeyi tersten saymayı deneyelim.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*M5btddEr_g_UbRmgSZpXRQ.png\" height=1000 width=400></center>\n",
    "\n",
    "Diğerine nazaran biraz daha zor oldu değil mi?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Son olarak olayı biraz daha zorlaştıralım ve F ile başlayıp devamını getirelim. Muhtemelen baştaki birkaç harfte zorlanacaksınız ama aklınıza gelince kalanı çok doğal şekilde devam edecek. Bu yüzden bunun zor olabilmesinin mantıklı bir nedeni var. Alfabeyi bir dizi olarak öğrenirsiniz. Sıralı bellek(Sequential memory), beyninizin sıra desenlerini tanımasını kolaylaştıran bir mekanizmadır.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*3o3Z7Roi2-8crmxRQYOoeg.png\" height=1000 width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tekrarlayan Sinir Ağları\n",
    "\n",
    "Pekala, RNN’lerde bu soyut sıralı bellek kavramı var, ama bir RNN bu kavramı nasıl tekrarlıyor? Şimdi ileri beslemeli sinir ağı olarak da bilinen geleneksel bir sinir ağına bakalım. Giriş katmanı, gizli katmanı ve çıkış katmanına sahibiz.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/112/1*IIWsi6jwUdt__-z1WpyqrA.png\" height=600 width=150></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T01:32:04.088081Z",
     "start_time": "2020-04-02T01:32:04.081100Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sinir ağına önceki bilgileri iletebilecek bir döngü eklersek ne olur? Tekrarlayan bir sinir ağının yaptığı şey budur. Bir RNN, bilginin bir adımdan diğerine akmasına izin vermek için bir otoyol görevi gören bir döngü mekanizmasına sahiptir.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/250/1*T_ECcHZWpjn0Ki4_4BEzow.gif\" height=800 width=300></center>\n",
    "\n",
    "Bu bilgi, önceki girdilerin bir temsili olan gizli durumdur (hidden state). Bunun nasıl çalıştığını daha iyi anlamak için RNN kullanım örneğini inceleyelim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bir Chatbot Eğitmek istediğimizi düşünelim. Bu Chatbot kullanıcıdan aldığı girdiden konu, niyeti tespit etmeye çalışsın. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/480/1*NLbr0TzqDz98QhMUYyX41A.gif\" height=1200 width=500></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bu sorunu çözmek için RNN Kullanarak Metin dizisini kodlayacağız. Sonrasında bu çıktıyı niyeti sınıflandıracak başka bir NN'e yollayacağız.\n",
    "\n",
    "Kullanıcıdan \"What time is it?\" Sorusunu aldığımızı düşünelim, öncelikle bu cümleyi kelimelere ayıracağız ve RNN'in sırayla çalışması sayesinde her seferinde bir kelime besleyeceğiz. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*G7T4sFO-1ByMepsa5OilsQ.gif\" height=1200 width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T01:49:46.827338Z",
     "start_time": "2020-04-02T01:49:46.822347Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "İlk adımda \"What\" kelimesini RNN'e besleyeceğiz ve RNN \"What\"ı Encode edip, bir çıktı üretecek. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*Qx6OiQnskfyCEzb8aZDgaA.gif\" height=1200 width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "İkinci adımda \"Time\" kelimesini RNN'e besleyeceğiz burada RNN hem \"Time\" hemde önceki gizli durumdan öğrendiği \"What\" kelimesi hakkında bilgiye sahip olacak. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*Qx6OiQnskfyCEzb8aZDgaA.gif\" height=1200 width=500></center>\n",
    "\n",
    "devamında ise kalan bütün kelimeleri sırayla besleyeceğiz. Son adımda RNN artık önceki bütün kelimelerin bilgisi ile beslenmiş olacak. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*d_POV7c8fzHbKuTgJzCxtA.gif\" height=1200 width=500></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:00:12.579715Z",
     "start_time": "2020-04-02T02:00:12.573697Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Son olarakta nihai çıktımız bütün cümleyi oluşturduğundan onuda niyet, konu tespiti yapacak NN'e beslemeliyiz.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*3bKRTcqSbto3CXfwshVwmQ.gif\" height=1200 width=500></center>\n",
    "\n",
    "Basit bir kodla'da yaptığımız şu şekilde bir işlem;\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/500/1*RQmHo9eJv1ZJa7P5FiyS9A.png\" height=1200 width=500></center>\n",
    "\n",
    "\n",
    "İlk olarak, ağ katmanlarınızı ve ilk gizli durumu başlatırsınız. Gizli durumu şekli ve boyutu, tekrarlayan sinir ağınızın şekline ve boyutuna bağlı olacaktır. Daha sonra girişleriniz arasında dolaşır, kelimeyi ve gizli durumu RNN'ye iletirsiniz. RNN çıktıyı ve değiştirilmiş gizli durumu döndürür. Kelimeleriniz bitene kadar dönmeye devam edersiniz. Son olarak çıktıyı feed forward katmanına geçirirseniz bir tahmin döndürür. Ve bu kadar! Tekrarlayan bir sinir ağının ileri geçişini yapmanın kontrol akışı bir for döngüsüdür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sorunlar\n",
    "\n",
    "### Vanishing Gradient (Kaybolan Eğim)\n",
    "Kaybolan eğim problemi büyük zaman serisi verileriyle çalıştığımızda ortaya çıkar. Aşağıdaki örnektede göreceğiz gibi RNN daha fazla adımı işledikçe, önceki adımlardan elde edilen bilgileri korumakta güçlük çeker. Gördüğünüz gibi, “ne” ve “zaman” kelimesinden gelen bilgiler, son zaman adımında neredeyse yoktur. \n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/138/1*yQzlE7JseW32VVU-xlOUvQ.png\" height=400 width=200></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bir sinir ağının eğitimi üç ana aşamaya sahiptir.\n",
    "\n",
    "- İlk olarak, bir ileri geçiş yapar ve bir tahmin yapar.\n",
    "- İkincisi, bir kayıp fonksiyonu kullanarak tahminleri gerçek değerlerle karşılaştırır. Kayıp fonksiyonu, ağın ne kadar kötü performans gösterdiğine dair bir tahmini bir hata değeri çıkarır. Son olarak, ağdaki her nöron için eğimleri hesaplayan geri yayılımı yapmak için bu hata değerini kullanır. Eğim ne kadar büyükse, ayarlamalar o kadar büyür ve tersi de geçerlidir. Sorun da burada yatmaktadır. Geri yayılımı yaparken, bir katmandaki her nöron, eğim etkileriyle ilgili olarak, önceki katmandaki eğim değerini hesaplar. Bu nedenle, önceki katmanlara yapılan ayarlamalar küçükse, geçerli katmana yapılan ayarlar daha da küçük olacaktır. Bu, geri yayıldıkça eğimin üssel olarak küçülmesine neden olur. Daha önceki katmanlar, iç ağırlıklar son derece küçük eğimleriyle ayarlanmakta olduğu için, herhangi bir öğrenme yapamazlar. Bu, kaybolan eğim problemini doğurur. Bu problemin çözümü içinde LSTM(Uzun Kısa Dönem Hafıza) yöntemi ortaya çıkmıştır.\n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/250/1*8eriEDJZisidMG_yyEDEAA.gif\" height=400 width=200></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uzun-Kısa Dönem Hafıza (LSTM)\n",
    "Genellikle “LSTM'ler” olarak adlandırılan Uzun Kısa Süreli Bellek ağları, uzun süreli bağımlılıkları öğrenebilen özel bir RNN türüdür. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*n-IgHZM5baBUjq0T7RYDBw.gif\" height=3000 width=1000></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tüm tekrarlayan sinir ağları(RNN), sinir ağının tekrarlayan modülleri zinciri şeklindedir. Standart RNN'lerde, bu tekrarlanan modül, tek tanh tabakası gibi çok basit bir yapıya sahiptir. \n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" height=3000 width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LSTM'ler de bu zincir benzeri yapıya sahiptir, ancak tekrarlayan modül farklı bir yapıya sahiptir. Tek bir sinir ağı katmanına sahip olmak yerine, çok özel bir şekilde etkileşime giren dört tane var.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/2062/1*XuM8aRzh17kdjeOgrjFE6A.png\" height=3000 width=1000></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM'in Arkasındaki Düşünce\n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" height=3000 width=1000></center>\n",
    "\n",
    "\n",
    "LSTM'lerin anahtarı, diyagramın üstünden geçen yatay çizgi olan hücre durumudur. Hücre durumu bir tür taşıma bandı gibidir. Sadece bazı küçük doğrusal etkileşimlerle tüm zincir boyunca aşağı doğru ilerler. Bilginin değişmeden akması çok kolaydır.\n",
    "\n",
    "LSTM, kapı adı verilen yapılar tarafından dikkatlice düzenlenmiş olan hücre durumunu çıkarma veya bilgi ekleme yeteneğine sahiptir. kapılar isteğe bağlı olarak bilgi aktarmanın bir yoludur. Bir sigmoid sinir ağı tabakası ve noktasal bir çarpma işleminden oluşurlar.\n",
    "\n",
    "Sigmoid katmanı, sıfır ile bir arasında sayılar çıkarır ve her bir bileşenden ne kadarının geçmesi gerektiğini açıklar. Sıfır değeri “hiçbirşeyi geçirme” anlamına gelirken, bir değeri “her şeyi geçir!” Anlamına gelir.\n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\" height=300 width=100></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adım Adım LSTM ve Matematiği\n",
    "\n",
    "#### Unutma Kapısı (Forget Gate)\n",
    "\n",
    "LSTM'mizin ilk adımı, hangi bilgilerin hücre durumundan atılacağına karar vermektir. Bu karar, “unutma kapısı katmanı” adı verilen bir sigmoid katman tarafından verilir. Ht − 1 ve xt'ye bakar ve Ct − 1 hücre durumundaki her sayı için 0 ile 1 arasında bir sayı verir. A 1 “bunu tamamen tut” u, 0 “tamamen bundan kurtul” u temsil eder.\n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" height=3000 width=1000></center>\n",
    "\n",
    "\n",
    "\n",
    "Önceki kelimeyi temel alarak bir sonraki kelimeyi tahmin etmeye çalışan bir dil modeli örneğimize geri dönelim. Böyle bir problemde, hücre durumu mevcut deneğin cinsiyetini içerebilir, böylece doğru zamirler kullanılabilir. Yeni bir konu gördüğümüzde, eski konunun cinsiyetini unutmak istiyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Girdi Kapısı (Input Gate) \n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" height=3000 width=1000></center>\n",
    "Bir sonraki adım, hücre durumunda hangi yeni bilgileri saklayacağımıza karar vermektir. Bunun iki kısmı vardır. İlk olarak, “giriş kapısı katmanı” adı verilen sigmoid katman hangi değerleri güncelleyeceğimize karar verir. Daha sonra tanh katmanı, duruma eklenebilecek yeni aday değerlerin (C ~ t) bir vektörünü oluşturur. Bir sonraki adımda, durumu güncellemek için bu ikisini birleştireceğiz.\n",
    "\n",
    "Dil modelimize örnek olarak, unuttuğumuz eskisinin yerini almak için yeni konunun cinsiyetini hücre durumuna eklemek istiyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hücre Durumu  (Cell State) \n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" height=3000 width=1000></center>\n",
    "\n",
    "Şimdi eski hücre durumu Ct − 1'i yeni hücre durumu Ct'ye güncelleme zamanı. Önceki adımlar zaten ne yapacağımıza karar verdi, sadece yapmamız gerekiyor.\n",
    "\n",
    "Eski durumu daha önce unutmaya karar verdiğimiz şeyleri unutarak, ft ile çarpıyoruz. Sonra bulduğumuz sonucu it ∗ Ct' ile topluyoruz. Bu, her bir devlet değerini ne kadar güncellemeye karar verdiğimize göre ölçeklendirilen yeni aday değerleridir.\n",
    "\n",
    "Dil modeli söz konusu olduğunda, önceki adımlarda karar verdiğimiz gibi eski konunun cinsiyeti hakkındaki bilgileri gerçekten bırakıp yeni bilgileri eklediğimiz yer burasıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Çıkış Kapısı (Output Gate) \n",
    "\n",
    "<center><img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" height=3000 width=1000></center>\n",
    "\n",
    "Son olarak, ne üreteceğimize karar vermemiz gerekiyor. Bu çıktı hücre durumumuza bağlı olacak, ancak filtrelenmiş bir versiyon olacak. İlk olarak, hücre durumunun hangi kısımlarının çıkacağına karar veren bir sigmoid katman çalıştırıyoruz. Sonra, hücre durumunu tanh'a koyduk (değerleri values1 ile 1 arasında olacak şekilde itmek için) ve sigmoid'in çıktısı ile çarpıyoruz, böylece sadece karar verdiğimiz parçaları çıkardık.\n",
    "\n",
    "Dil modeli örneği için, sadece bir konuyu gördüğünden, bundan sonra olacaksa, bir fiille ilgili bilgileri çıkarmak isteyebilir. Örneğin, öznenin tekil mi yoksa çoğul mu olduğu ortaya çıkabilir, böylece bir sonraki fiil buysa, bir fiilin hangi şekle konjuge edilmesi gerektiğini biliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adım Adım LSTM'in Animasyonlu Hali\n",
    "\n",
    "#### Unutma Kapısı (Forget Gate)\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*GjehOa513_BgpDDP6Vkw2Q.gif\" height=3000 width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Girdi Kapısı (Input Gate)\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*TTmYy7Sy8uUXxUXfzmoKbA.gif\" height=3000 width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hücre Durumu (Cell State)\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*S0rXIeO_VoUVOyrYHckUWg.gif\" height=3000 width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Çıkış Kapısı (Output Gate)\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*VOXRGhOShoWWks6ouoDN3Q.gif\" height=3000 width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T12:15:01.939586Z",
     "start_time": "2020-04-02T12:15:01.931607Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1400/1*qn_quuUSYzozyH3CheoQsA.png\" height=3000 width=1000></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
